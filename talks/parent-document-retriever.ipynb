{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2870102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_classic.retrievers import ParentDocumentRetriever\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain_community.storage import SQLStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "import frontmatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f2fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "deployment = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT\")\n",
    "embeddings_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-06-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e394a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSTGRES_URL = os.getenv(\"POSTGRES_URL\")\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\", \"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f54a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = AzureOpenAIEmbeddings(\n",
    "        azure_endpoint=endpoint,\n",
    "        api_key=api_key,\n",
    "        azure_deployment=embeddings_deployment,\n",
    "        api_version=api_version,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f20c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        api_key=api_key,\n",
    "        azure_deployment=deployment,\n",
    "        api_version=api_version,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd673f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2e1f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLES_DIR = Path(\"../data/articles\")\n",
    "DEFAULT_QUERY = \"How do cancellations impact SaaS growth?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3833b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ArticleMetadata:\n",
    "    article_id: str\n",
    "    title: str\n",
    "    source_path: str\n",
    "    url: str | None\n",
    "    author: str | None\n",
    "    date: str | None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db35dd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_files = [\n",
    "    ARTICLES_DIR / name\n",
    "    for name in sorted(os.listdir(ARTICLES_DIR))\n",
    "    if name.endswith(\".md\")\n",
    "]\n",
    "len(article_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d755e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles: list[Document] = []\n",
    "for path in article_files:\n",
    "    post = frontmatter.load(path)\n",
    "    metadata = {key: str(value) for key, value in post.metadata.items()}\n",
    "\n",
    "    stem = path.stem\n",
    "    article_id = stem.split(\"_\", 1)[0]\n",
    "    title = metadata.get(\"title\") or stem.replace(\"_\", \" \")\n",
    "\n",
    "    articles.append(\n",
    "        Document(\n",
    "            page_content=post.content.lstrip(),\n",
    "            metadata={\n",
    "                \"article_id\": article_id,\n",
    "                \"title\": title,\n",
    "                \"source_path\": str(path),\n",
    "                \"url\": metadata.get(\"url\"),\n",
    "                \"author\": metadata.get(\"author\"),\n",
    "                \"date\": metadata.get(\"date\"),\n",
    "            },\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40f82de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview(text: str, limit: int = 220) -> str:\n",
    "    compact = \" \".join(text.split())\n",
    "    return compact if len(compact) <= limit else compact[: limit - 3] + \"...\"\n",
    "\n",
    "\n",
    "def show_chunks(chunks: list[Document], title: str) -> None:\n",
    "    print(\"\\n\" + title)\n",
    "    for i, chunk in enumerate(chunks, start=1):\n",
    "        meta = chunk.metadata\n",
    "        print(\n",
    "            f\"{i}. Article {meta.get('article_id')} | chunk {meta.get('chunk_index')}: \"\n",
    "            f\"{preview(chunk.page_content)}\"\n",
    "        )\n",
    "\n",
    "def group_by_article(chunks: list[Document]) -> dict[str, list[int]]:\n",
    "    grouped: dict[str, list[int]] = {}\n",
    "    for chunk in chunks:\n",
    "        grouped.setdefault(chunk.metadata.get(\"article_id\", \"?\"), []).append(\n",
    "            int(chunk.metadata.get(\"chunk_index\", 0))\n",
    "        )\n",
    "    for key in grouped:\n",
    "        grouped[key] = sorted(grouped[key])\n",
    "    return grouped\n",
    "\n",
    "def print_queries_markdown_table(queries: list[str]) -> None:\n",
    "    print(\"| pos | content | words | chars |\")\n",
    "    print(\"|---:|---|---:|---:|\")\n",
    "    for i, q in enumerate(queries, start=1):\n",
    "        words = len(q.split())\n",
    "        chars = len(q)\n",
    "        safe_q = q.replace(\"|\", \"\\\\|\")\n",
    "        print(f\"| {i} | {safe_q} | {words} | {chars} |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd3c5fb",
   "metadata": {},
   "source": [
    "## Tools to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7b7334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "class WeatherInput(BaseModel):\n",
    "    \"\"\"Input for weather queries.\"\"\"\n",
    "    location: str = Field(description=\"City name or coordinates\")\n",
    "    units: Literal[\"celsius\", \"fahrenheit\"] = Field(\n",
    "        default=\"celsius\",\n",
    "        description=\"Temperature unit preference\"\n",
    "    )\n",
    "    include_forecast: bool = Field(\n",
    "        default=False,\n",
    "        description=\"Include 5-day forecast\"\n",
    "    )\n",
    "\n",
    "@tool(args_schema=WeatherInput)\n",
    "def get_weather(location: str, units: str = \"celsius\", include_forecast: bool = False) -> str:\n",
    "    \"\"\"Get current weather and optional forecast.\"\"\"\n",
    "    temp = 22 if units == \"celsius\" else 72\n",
    "    result = f\"Current weather in {location}: {temp} degrees {units[0].upper()}\"\n",
    "    if include_forecast:\n",
    "        result += \"\\nNext 5 days: Sunny\"\n",
    "    return result\n",
    "\n",
    "llm_with_tools = llm.bind_tools([get_weather])\n",
    "\n",
    "# Inspect the stored tool schemas immediately\n",
    "print(json.dumps(llm_with_tools.kwargs, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf2110d",
   "metadata": {},
   "source": [
    "## Naive RAG - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc194a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_splitter = CharacterTextSplitter(\n",
    "    separator=\"\",\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "static_chunks: list[Document] = []\n",
    "for doc in articles:\n",
    "    pieces = static_splitter.split_text(doc.page_content)\n",
    "    for i, text in enumerate(pieces, start=1):\n",
    "        static_chunks.append(\n",
    "            Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    **doc.metadata,\n",
    "                    \"chunk_index\": i,\n",
    "                    \"chunk_uid\": f\"{doc.metadata['article_id']}:{i}\",\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "len(static_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f601431a",
   "metadata": {},
   "source": [
    "### Connecting vector store\n",
    "\n",
    "If your collection is empty, start with the commented code, to create a new qdrant collection from our chunks by a method `from_documents()`. If you already have a collection, continue to use `from_existing_collection()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782337ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = QdrantClient(url=QDRANT_URL)\n",
    "\n",
    "# vectorstore = QdrantVectorStore.from_documents(\n",
    "#     static_chunks,\n",
    "#     collection_name=\"step1_chunks\",\n",
    "#     embedding=embeddings,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394699c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = QdrantVectorStore.from_existing_collection(\n",
    "    collection_name=\"step1_chunks\",\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5165509",
   "metadata": {},
   "outputs": [],
   "source": [
    "step1_results = vectorstore.similarity_search(\"How do cancellations impact SaaS growth?\", k=5)\n",
    "show_chunks(step1_results, \"Step 1: Naive RAG (chunk_size=400, k=5)\")\n",
    "print(\"\\nProblem: chunks are mid-article and missing context.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.similarity_search(\"How do cancellations impact SaaS growth?\", k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8c64c4",
   "metadata": {},
   "source": [
    "## Naive RAG - 10\n",
    "\n",
    "In this example, we are reusing the previous collection, but we want to show 10 most similar chunks, not just 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876faefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "step2_results = vectorstore.similarity_search(\"How do cancellations impact SaaS growth?\", k=10)\n",
    "print(\"\\nStep 2: Increase k to 10\")\n",
    "\n",
    "\n",
    "for article_id, indices in group_by_article(step2_results).items():\n",
    "    print(f\"Article {article_id}: chunks {indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d9dae6",
   "metadata": {},
   "source": [
    "## Multi-Query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "711ec6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| pos | content | words | chars |\n",
      "|---:|---|---:|---:|\n",
      "| 1 | How do customer cancellations and churn rates directly affect SaaS growth in terms of MRR and ARR, and how should I calculate their impact on customer lifetime value and unit economics? What models can I use to translate cancellation activity into projected revenue loss and slowed growth trajectories? | 48 | 302 |\n",
      "| 2 | In a subscription SaaS business, what is the relationship between cancellation behavior and growth velocity, and which retention and cohort metrics best quantify the drag caused by cancellations? How can I track and report net revenue retention, gross churn, and expansion MRR to understand the real effect on growth? | 49 | 317 |\n",
      "| 3 | What operational and strategic changes reduce the negative impact of cancellations on SaaS growth, such as improvements in onboarding, pricing, product-market fit, or customer success programs? How do cancellations influence CAC payback period, churn-adjusted LTV, and long-term growth forecasting, and what interventions most effectively reverse those trends? | 47 | 360 |\n",
      "| 4 | How do cancellations impact SaaS growth? | 6 | 40 |\n"
     ]
    }
   ],
   "source": [
    "ORIGINAL_QUERY = \"How do cancellations impact SaaS growth?\"\n",
    "rewrite_prompt = (\n",
    "    \"Generate exactly 3 diverse query rewrites for semantic search. Each query should be longer than 1 sentence to optimize the cosine similarity. Return each query on its own line. No numbering\"\n",
    ")\n",
    "rewrites = llm.invoke(\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": rewrite_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Original query: {ORIGINAL_QUERY}\"},\n",
    "    ]\n",
    ").content\n",
    "queries = [line.strip() for line in rewrites.splitlines() if line.strip()]\n",
    "queries = queries[:3]\n",
    "queries.append(ORIGINAL_QUERY)\n",
    "\n",
    "print_queries_markdown_table(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da7ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_chunks: list[Document] = []\n",
    "for q in queries:\n",
    "    multi_chunks.extend(vectorstore.similarity_search(q, k=10))\n",
    "    \n",
    "unique: dict[str, Document] = {}\n",
    "for chunk in multi_chunks:\n",
    "    uid = chunk.metadata.get(\"chunk_uid\")\n",
    "    if uid and uid not in unique:\n",
    "        unique[uid] = chunk\n",
    "        \n",
    "multi_results = list(unique.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a208c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStep 4: Multi-query (4 variations x 10) grouped by article\")\n",
    "for article_id, indices in group_by_article(multi_results).items():\n",
    "    print(f\"Article {article_id}: chunks {indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d61ebc5",
   "metadata": {},
   "source": [
    "## Big Chunks\n",
    "\n",
    "In this example, we are chunking on 2000 characters, to have more knowledge in each of the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfca0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "big_chunks: list[Document] = []\n",
    "for doc in articles:\n",
    "    pieces = big_splitter.split_text(doc.page_content)\n",
    "    for i, text in enumerate(pieces, start=1):\n",
    "        big_chunks.append(\n",
    "            Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    **doc.metadata,\n",
    "                    \"chunk_index\": i,\n",
    "                    \"chunk_uid\": f\"{doc.metadata['article_id']}:{i}\",\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "len(big_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e5880c",
   "metadata": {},
   "source": [
    "### Create new vector store\n",
    "\n",
    "If you have not yet ingested the big chunks into the vector store - proceed with the `from_documents()`. If you have already documents in this collection, use `from_existing_collection()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = QdrantVectorStore.from_documents(\n",
    "    big_chunks,\n",
    "    collection_name=\"step5_chunks\",\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d34f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = QdrantVectorStore.from_existing_collection(\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"step5_chunks\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecfc436",
   "metadata": {},
   "source": [
    "### Execute retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3431cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "step5_results = vectorstore.similarity_search(\"How do cancellations impact SaaS growth?\", k=5)\n",
    "show_chunks(step5_results, \"Step 5: Naive fix (chunk_size=2000)\")\n",
    "print(\"\\nEmbedding signal is diluted; relevance drops.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859cf21d",
   "metadata": {},
   "source": [
    "## Parent Document Retriever\n",
    "\n",
    "We are connecting now big chunks (in this case we do not chunk articles, but return full length) with small chunks for the retrieval purpose. We are using ParentDocumentRetriever, where we store big documents in Postgres database, and small chunks inside qdrant vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe1640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_store = SQLStore(namespace=\"pdr_parent_docs\", db_url=POSTGRES_URL)\n",
    "byte_store.create_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d111c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "parent_splitter = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e27e92",
   "metadata": {},
   "source": [
    "### Creating vector store\n",
    "\n",
    "ParentDocumentRetriever requires us to pass already initialized vector store. Because we use qdrant, we need to first create the new collection, it will be ingesting data using `collection_exists()`. If the collection is not yet created, we will create it using `create_collection()`, according to our embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dce3719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(url=QDRANT_URL)\n",
    "is_collection_existing = client.collection_exists(\n",
    "    collection_name=\"pdr_child_chunks\",\n",
    ")\n",
    "\n",
    "if not is_collection_existing:\n",
    "    vector_dim = len(embeddings.embed_query(\"dimension probe\"))\n",
    "\n",
    "    client.create_collection(\n",
    "        collection_name=\"pdr_child_chunks\",\n",
    "        vectors_config=VectorParams(size=vector_dim, distance=Distance.COSINE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6e3a9472",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"pdr_child_chunks\",\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e750b65",
   "metadata": {},
   "source": [
    "### Creating and ingesting retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff40f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=byte_store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    "    child_metadata_fields=[\"article_id\", \"title\", \"source_path\", \"url\"],\n",
    "    search_kwargs={\"k\": 10},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c805f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.add_documents(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcab8ca5",
   "metadata": {},
   "source": [
    "### Using the ParentDocument Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af8ffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_results = retriever.invoke(\"How do cancellations impact SaaS growth?\")\n",
    "\n",
    "print(\"\\nStep 6: Parent Document Retriever (Qdrant + Postgres)\")\n",
    "for i, doc in enumerate(parent_results, start=1):\n",
    "    meta = doc.metadata\n",
    "    print(\n",
    "        f\"{i}. Article {meta.get('article_id')} | {meta.get('title')} | chars: {len(doc.page_content)}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced-rag-talks (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
